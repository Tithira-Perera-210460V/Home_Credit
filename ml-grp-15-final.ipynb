{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef481c17",
   "metadata": {
    "papermill": {
     "duration": 0.022918,
     "end_time": "2024-05-11T07:34:44.031255",
     "exception": false,
     "start_time": "2024-05-11T07:34:44.008337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Machine Leaning Problem\n",
    ">The absence of a credit history might mean a lot of things, including young age or a preference for cash. Without traditional data, someone with little to no credit history is likely to be denied. Consumer finance providers must accurately determine which clients can repay a loan and which cannot and data is key. If data science could help better predict oneâ€™s repayment capabilities, loans might become more accessible to those who may benefit from them the most.\n",
    "Currently, consumer finance providers use various statistical and machine learning methods to predict loan risk. These models are generally called scorecards. In the real world, clients' behaviors change constantly, so every scorecard must be updated regularly, which takes time. The scorecard's stability in the future is critical, as a sudden drop in performance means that loans will be issued to worse clients on average. The core of the issue is that loan providers aren't able to spot potential problems any sooner than the first due dates of those loans are observable. Given the time it takes to redevelop, validate, and implement the scorecard, stability is highly desirable. There is a trade-off between the stability of the model and its performance, and a balance must be reached before deployment.\n",
    "Founded in 1997, competition host Home Credit is an international consumer finance provider focusing on responsible lending primarily to people with little or no credit history. Home Credit broadens financial inclusion for the unbanked population by creating a positive and safe borrowing experience. We previously ran a competition with Kaggle that you can see here.\n",
    "Your work in helping to assess potential clients' default risks will enable consumer finance providers to accept more loan applications. This may improve the lives of people who have historically been denied due to lack of credit history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef19b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:44.080841Z",
     "iopub.status.busy": "2024-05-11T07:34:44.078966Z",
     "iopub.status.idle": "2024-05-11T07:34:50.299592Z",
     "shell.execute_reply": "2024-05-11T07:34:50.297763Z"
    },
    "papermill": {
     "duration": 6.248599,
     "end_time": "2024-05-11T07:34:50.303242",
     "exception": false,
     "start_time": "2024-05-11T07:34:44.054643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import  train_test_split,TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR       = ROOT / \"csv_files\" / \"train\" \n",
    "TEST_DIR        = ROOT / \"csv_files\" / \"test\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a5460",
   "metadata": {
    "papermill": {
     "duration": 0.024026,
     "end_time": "2024-05-11T07:34:50.350432",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.326406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b7a41",
   "metadata": {
    "papermill": {
     "duration": 0.025645,
     "end_time": "2024-05-11T07:34:50.401239",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.375594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Aggregration*\n",
    "> Tables that are depth 1 and depth 2 will be handled in this section\n",
    "\n",
    "Currently the mean, max and last statistics are used to aggregrate. But it is possible to use more methods. Refer the polars documentation  here https://docs.pola.rs/py-polars/html/reference/dataframe/aggregation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96400180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:50.462795Z",
     "iopub.status.busy": "2024-05-11T07:34:50.461516Z",
     "iopub.status.idle": "2024-05-11T07:34:50.487589Z",
     "shell.execute_reply": "2024-05-11T07:34:50.485946Z"
    },
    "papermill": {
     "duration": 0.059728,
     "end_time": "2024-05-11T07:34:50.490998",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.431270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    \"\"\" This class handles the aggregration of the columns in depth > 0 columns\n",
    "    \"\"\"\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        \"\"\"Handles the aggregrated new columns for columns which are \"P\" or \"A\" type\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        \n",
    "        # Select the columns with the P and A suffixes\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        \n",
    "        # Create a column conatining the max value of the each selected column\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        \n",
    "        # Create a column conatining the last value of the each selected column\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        \n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        \n",
    "        # Create a column conatining the maen value of the each selected column\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max +expr_last+expr_mean\n",
    "    \n",
    "    def date_expr(df):\n",
    "        \"\"\"Handles the aggregrated new columns for columns which are \"D\" type\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        \n",
    "        # Select the columns with the D suffix\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        \n",
    "        # Select the columns with the \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last+expr_mean\n",
    "    \n",
    "    def str_expr(df):\n",
    "        \"\"\"Handles the aggregrated new columns for columns which are \"M\" type\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last#+expr_count\n",
    "    \n",
    "    def other_expr(df):\n",
    "        \"\"\"Handles the aggregrated new columns for columns which are \"T\" or \"L\" type\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last\n",
    "    \n",
    "    def count_expr(df,filename):\n",
    "        \"\"\"Handles the aggregrated new columns for columns \"num_group1\" and \"num_group2\"\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        file_parts = filename.split(\"_\")\n",
    "        filename =  \"\"\n",
    "        for part in file_parts[1:]:\n",
    "            filename += part\n",
    "            filename += \"_\"\n",
    "            \n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"{str(filename).split('/')[-1][:-4]}max_{col}\") for col in cols] \n",
    "        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"{str(filename).split('/')[-1][:-4]}last_{col}\") for col in cols]\n",
    "        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return  expr_max +expr_last\n",
    "    \n",
    "    def get_exprs(df,filename):\n",
    "        \"\"\"Aggregrate the entire dataframe\n",
    "        \n",
    "        Args : \n",
    "        -df: not aggregrated dataframe (Polars object)\n",
    "        \n",
    "        Return:\n",
    "        - Polars DataFrame: aggregrated new columns\n",
    "        \"\"\"\n",
    "        \n",
    "     \n",
    "        # Add all aggregrated columns to create the aggregrated dataframe\n",
    "        exprs = Aggregator.num_expr(df) +  Aggregator.date_expr(df) + Aggregator.str_expr(df) + Aggregator.other_expr(df) + Aggregator.count_expr(df,filename)\n",
    "        return exprs\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f01cc8",
   "metadata": {
    "papermill": {
     "duration": 0.030054,
     "end_time": "2024-05-11T07:34:50.547806",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.517752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Concatination\n",
    "> Tables that are divided into several files will be concatinated in this section.\n",
    "\n",
    "Handling the credit_bureau_a_1 and credit_bureau_a_2 was a nightmare. I initially tried to load only the important features which is about 10 features for the both tables. And yet it ran out memory during the aggregration stage.It is reasonable as for 10 features it creates 90 features when the depth is 2. So my approch was to load the specified columns and do the aggregration before the concatination. And it worked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba64b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:50.602010Z",
     "iopub.status.busy": "2024-05-11T07:34:50.601472Z",
     "iopub.status.idle": "2024-05-11T07:34:50.634111Z",
     "shell.execute_reply": "2024-05-11T07:34:50.633140Z"
    },
    "papermill": {
     "duration": 0.060798,
     "end_time": "2024-05-11T07:34:50.637539",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.576741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concatinate(filename):\n",
    "    \"\"\"\n",
    "    Given the filepath of this concatinate all the files that associated with it\n",
    "    \n",
    "    Args : \n",
    "    filename : Filepath to be aggregrated\n",
    "\n",
    "    Return:\n",
    "    Polars DataFrame: Concatinated Tables\n",
    "    \n",
    "    \"\"\"\n",
    "    filename = str(filename)\n",
    "    path = \"\"\n",
    "    if filename.split('/')[-1][:2] == \"tr\":\n",
    "        file_type = \"train\"\n",
    "        path = TRAIN_DIR\n",
    "    elif filename.split('/')[-1][:2] == \"te\":\n",
    "        file_type = \"test\"\n",
    "        path = TEST_DIR\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized file {filename.split('/')[-1]}\")\n",
    "    \n",
    "    file_groups = {\n",
    "        f\"{file_type}_applprev_1_0.csv\" : [\n",
    "            f\"{file_type}_applprev_1_1.csv\",\n",
    "            f\"{file_type}_applprev_1_2.csv\" \n",
    "        ],\n",
    "        f\"{file_type}_credit_bureau_a_1_0.csv\": [\n",
    "            f\"{file_type}_credit_bureau_a_1_1.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_1_2.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_1_3.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_1_4.csv\"\n",
    "        ],\n",
    "        f\"{file_type}_credit_bureau_a_2_0.csv\": [\n",
    "            f\"{file_type}_credit_bureau_a_2_1.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_2.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_3.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_4.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_5.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_6.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_7.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_8.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_9.csv\",\n",
    "            f\"{file_type}_credit_bureau_a_2_10.csv\",\n",
    "        ],\n",
    "        f\"{file_type}_static_0_0\": [\n",
    "            f\"{file_type}_static_0_1\",\n",
    "            f\"{file_type}_static_0_2\"\n",
    "        ],  \n",
    "    }\n",
    "    \n",
    "    selected_features = {\n",
    "        f\"{file_type}_credit_bureau_a_1_0.csv\": [\n",
    "            'case_id',\n",
    "            'dateofcredend_289D',\n",
    "            'refreshdate_3813885D',\n",
    "            'numberofinstls_320L',\n",
    "            'numberofcontrsvalue_358L',\n",
    "            'nominalrate_281L',\n",
    "            'numberofoutstandinstls_59L',\n",
    "            'numberofoverdueinstlmax_1039L',\n",
    "            'num_group1'\n",
    "        ],\n",
    "        f\"{file_type}_credit_bureau_a_2_0.csv\": [\n",
    "            'case_id',\n",
    "            'collater_typofvalofguarant_407M',\n",
    "            'collaterals_typeofguarante_359M',\n",
    "            'subjectroles_name_541M',\n",
    "            'collater_typofvalofguarant_298M',\n",
    "            'collaterals_typeofguarante_669M',\n",
    "            'subjectroles_name_838M',\n",
    "            'pmts_year_1139T',\n",
    "            'pmts_year_507T',\n",
    "            'num_group2',\n",
    "            'num_group1'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if filename.split('/')[-1] in list(selected_features.keys()):\n",
    "        concatinated_df = pl.read_csv(f\"{filename}\", null_values=\"a55475b1\", columns=selected_features[filename.split('/')[-1]])\n",
    "        if \"num_group2\" in concatinated_df.columns:\n",
    "            concatinated_df = concatinated_df.group_by(\"case_id\",\"num_group1\").agg(Aggregator.get_exprs(concatinated_df,filename.split('/')[-1])).sort(\"case_id\")\n",
    "            concatinated_df = concatinated_df.group_by(\"case_id\").agg(Aggregator.get_exprs(concatinated_df, filename.split('/')[-1])).sort(\"case_id\")\n",
    "        \n",
    "        elif \"num_group1\" in concatinated_df.columns:\n",
    "            concatinated_df = concatinated_df.group_by(\"case_id\").agg(Aggregator.get_exprs(concatinated_df,filename.split('/')[-1])).sort(\"case_id\")\n",
    "    else:\n",
    "        concatinated_df = pl.read_csv(f\"{filename}\", null_values=\"a55475b1\")\n",
    "\n",
    "        \n",
    "    \n",
    "    if filename.split('/')[-1] in list(file_groups.keys()):\n",
    "        \n",
    "        \n",
    "            \n",
    "        for file in file_groups[f\"{filename.split('/')[-1]}\"]:\n",
    "            \n",
    "            if file not in os.listdir(path):\n",
    "                continue\n",
    "            \n",
    "            df = pl.read_csv(f\"{path}/{file}\", null_values=\"a55475b1\") \n",
    "            if \"num_group2\" in df.columns:\n",
    "                df = df.group_by(\"case_id\",\"num_group1\").agg(Aggregator.get_exprs(df,filename.split('/')[-1])).sort(\"case_id\")\n",
    "                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df, filename.split('/')[-1])).sort(\"case_id\")\n",
    "        \n",
    "            elif \"num_group1\" in df.columns:\n",
    "                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df,filename.split('/')[-1])).sort(\"case_id\")\n",
    "\n",
    "            \n",
    "            concatinated_df = pl.concat([concatinated_df, df], how=\"diagonal_relaxed\")\n",
    " \n",
    "    \n",
    "    return concatinated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841617f",
   "metadata": {
    "papermill": {
     "duration": 0.022275,
     "end_time": "2024-05-11T07:34:50.682122",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.659847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Handling Invaluble Attributes\n",
    "> Columns that have higher null value ratio might give higher error when it is imputed. Therefore those columns need to be removed. Also there might be columns that has only one unique values or one unique value and missing values (After imputing there will be only one unique value). Since these do not provide any insights to the model we can remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749ca08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:50.733933Z",
     "iopub.status.busy": "2024-05-11T07:34:50.733483Z",
     "iopub.status.idle": "2024-05-11T07:34:50.747650Z",
     "shell.execute_reply": "2024-05-11T07:34:50.746638Z"
    },
    "papermill": {
     "duration": 0.039689,
     "end_time": "2024-05-11T07:34:50.750263",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.710574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_columns_with_high_missing_values(df, threshold=50):\n",
    "    \"\"\"\n",
    "    Drop columns with missing values exceeding a given threshold percentage.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame: Input DataFrame\n",
    "    - threshold: float: Threshold percentage for missing values\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Modified DataFrame with columns dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "        \n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = (df.isna().sum() / len(df)) * 100\n",
    "\n",
    "    # Filter columns with missing percentage greater than the threshold\n",
    "    columns_to_drop = missing_percentage[missing_percentage > threshold].index\n",
    "\n",
    "    # Drop columns with more than the threshold percentage of missing values\n",
    "    df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Display information about dropped columns\n",
    "#     print(\"Columns dropped due to more than {}% missing values:\".format(threshold))\n",
    "#     print(columns_to_drop)\n",
    "\n",
    "#     # Display information about the cleaned DataFrame\n",
    "#     print(\"Shape of cleaned DataFrame:\", df_cleaned.shape)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def remove_single_unique_value_columns(df):\n",
    "    \"\"\"Remove the columns with only one unique value as it does not provide any insights to the model.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame: Input Polars DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Modified Polars DataFrame with columns dropped    \n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "    unique_value_counts = df.nunique()\n",
    "    single_unique_value_columns = unique_value_counts[unique_value_counts == 1].index\n",
    "    df = df.drop(columns=single_unique_value_columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_dominated_columns(df, threshold=0.8):\n",
    "    \"\"\"Remove the columns that has higher amount of null values than the given threshold.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame: Input Polars DataFrame\n",
    "    - threshold: Maximum null value presentage\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Modified Polars DataFrame with columns dropped    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "        \n",
    "    num_rows = len(df)\n",
    "    dominated_columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Skip 'case_id' column\n",
    "        if col == 'case_id':\n",
    "            continue\n",
    "        \n",
    "        value_counts = df[col].value_counts()\n",
    "        dominant_value_count = value_counts.max()\n",
    "        dominant_value_percentage = dominant_value_count / num_rows\n",
    "        \n",
    "        if dominant_value_percentage >= threshold:\n",
    "            dominated_columns.append(col)\n",
    "    \n",
    "    df = df.drop(columns=dominated_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a897339",
   "metadata": {
    "papermill": {
     "duration": 0.023853,
     "end_time": "2024-05-11T07:34:50.797543",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.773690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 Handling Missing Values*\n",
    "> This section responsible for removing the null values.\n",
    "\n",
    "As you know there are three ways to remove missing values. We can try doing other ways\n",
    "*     Dropping null rows/columns\n",
    "*     Imputing using a statistic (Current Method)\n",
    "*     **Using a ML model to predic the values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff71dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:50.844341Z",
     "iopub.status.busy": "2024-05-11T07:34:50.843494Z",
     "iopub.status.idle": "2024-05-11T07:34:50.853092Z",
     "shell.execute_reply": "2024-05-11T07:34:50.852195Z"
    },
    "papermill": {
     "duration": 0.035406,
     "end_time": "2024-05-11T07:34:50.855656",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.820250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in categorical columns with mode and numerical columns with mean.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame: Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with missing values handled using imputation\n",
    "    \"\"\"\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object','category']).columns\n",
    "    numerical_columns = df.select_dtypes(exclude=['object','category']).columns\n",
    "\n",
    "    # Impute missing values for categorical columns with the most frequent value (mode)\n",
    "    for col in categorical_columns:\n",
    "        if df[col].isna().sum() != len(df):\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(np.nan, inplace=True)\n",
    "\n",
    "    # Impute missing values for numerical columns with the mean\n",
    "    for col in numerical_columns:\n",
    "        if df[col].isna().sum() != len(df):\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(np.nan, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c89ff8",
   "metadata": {
    "papermill": {
     "duration": 0.027819,
     "end_time": "2024-05-11T07:34:50.906172",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.878353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.5 Handling Datatypes to Optimize the Memory\n",
    "> Some columns are not assigned properly in the dataframe. In the first part of this sectiion those columns will be converted to corresponding data type according to the data it carries or according to the column suffix. A drawback of this process is it does not optimize the assigned data type (Eg: An integer column that has values which can be interpreted using only 8 bits will be assigned 64 bits.). This issue will be handled in the second part od the section where it checks the minimum  and the maximun values of the column and assign the corresponding data subtype.  \n",
    "\n",
    "Category datatype reduces the memory consumption of the nominal data in most cases. Check out what is going in there.                                           https://medium.com/analytics-vidhya/unleash-the-power-of-pandas-category-dtype-encode-categorical-data-in-smarter-ways-eb787cd274df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b0fa47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:50.959409Z",
     "iopub.status.busy": "2024-05-11T07:34:50.958591Z",
     "iopub.status.idle": "2024-05-11T07:34:50.981056Z",
     "shell.execute_reply": "2024-05-11T07:34:50.980051Z"
    },
    "papermill": {
     "duration": 0.054295,
     "end_time": "2024-05-11T07:34:50.983749",
     "exception": false,
     "start_time": "2024-05-11T07:34:50.929454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_data_types(df, pandas = False):\n",
    "    \"\"\"Convert the column data types according to thier suffixes and change it back to optimize the memory\n",
    "    \n",
    "    Args:\n",
    "    - df: Input DataFrame (Polars or Pandas)\n",
    "    - pandas: If True return the pandas dataframe else return the polar dataframe. Default False\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame (Polaris or Pandas) with correct and optimised data types \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convert integer columns to integer columns\n",
    "        if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\",\"target\"]:\n",
    "            df[col] = df[col].astype(np.int64) \n",
    "            \n",
    "        # Convert date columns to corresponding timestamps\n",
    "        elif col in [\"date_decision\"] or  col[-1] in (\"D\",):\n",
    "            # Handles imputed date columns. Faster\n",
    "#             df[col] = pd.to_datetime(df[col]).apply(lambda x: int(x.timestamp()))\n",
    "            # Handles missing value one by one. Slower\n",
    "            df[col] = pd.to_datetime(df[col]).apply(lambda x: int(x.timestamp()) if not pd.isnull(x) else pd.NaT)\n",
    "        \n",
    "        # Convert numerical columns to float columns\n",
    "        elif col[-1] in (\"P\", \"A\"):\n",
    "            df[col] = df[col].astype(np.float64) \n",
    "        \n",
    "        # Convert masked data columns to category data type \n",
    "        elif col[-1] in (\"M\"):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "            \n",
    "        # If there are columns with strings try converting it to the categorey columns\n",
    "        elif df[col].dtype == \"string\" or \"object\":\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "            \n",
    "        elif df[col].dtype == \"datetime64\":\n",
    "            df[col] = pd.to_datetime(df[col]).apply(lambda x: int(x.timestamp()) if not pd.isnull(x) else pd.NaT)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Converting String or objects to category can reduce memory space. But when a String or Object column \n",
    "        # has null values, it does not convert into category type. Threrefore it is best practice to first \n",
    "        # impute the null values and then transform the data types\n",
    "        \n",
    "    \n",
    "    # Iterate throgh the columns    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # find the column data type\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # If the type is \"category\" do not optimize\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        # If the type is not \"object\" optimize\n",
    "        if col_type != object and col_type == \"datetime64\":\n",
    "            \n",
    "            # min and max of the numerical values\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if it an integer\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Check whether it is possible to reduce the bit amount of the type by checking the min max values\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            # Handles floats\n",
    "            else:\n",
    "                print(df[col].dtype)\n",
    "                # Check whether it is possible to reduce the bit amount of the type by checking the min max values\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "        \n",
    "        # whatever the other data types do not optimize\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # If panda attibite is True return the corresponding pandas dataframe else return the polars dataframe\n",
    "    if pandas:\n",
    "        return df\n",
    "    else:\n",
    "        return pl.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc6e7b",
   "metadata": {
    "papermill": {
     "duration": 0.021813,
     "end_time": "2024-05-11T07:34:51.026827",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.005014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.6 Data Reduction*\n",
    "> Data reduction can be carried out to decrease the memory consumption. Currently an attributes selection is carried out using the correlation for numerical columns\n",
    "\n",
    "Can try out other attribute data methods like PCA, t-sne, SVD. Or can try out other attribute selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19621c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:51.072398Z",
     "iopub.status.busy": "2024-05-11T07:34:51.071269Z",
     "iopub.status.idle": "2024-05-11T07:34:51.080447Z",
     "shell.execute_reply": "2024-05-11T07:34:51.079505Z"
    },
    "papermill": {
     "duration": 0.035128,
     "end_time": "2024-05-11T07:34:51.083035",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.047907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_data(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe remove the correlated attributes\n",
    "    \n",
    "    Args:\n",
    "    -df : Pandas dataframe \n",
    "    \n",
    "    Return:\n",
    "    -pandas.DataFrame : Filtered pandas dataframe\n",
    "    \"\"\"\n",
    "    numerics = ['int8','int16', 'int32', 'int64','float16', 'float32', 'float64']\n",
    "    numerical_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in numerics:\n",
    "            numerical_cols.append(col)\n",
    "            \n",
    "    cor_matrix = df[numerical_cols].corr()\n",
    "    columns_to_drop = set()\n",
    "    for i in range(len(cor_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(cor_matrix.iloc[i, j]) > 0.8:\n",
    "                colname = cor_matrix.columns[i]\n",
    "                columns_to_drop.add(colname)\n",
    "    df_filtered = df.drop(columns=columns_to_drop)\n",
    "    return df_filtered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d26f68",
   "metadata": {
    "papermill": {
     "duration": 0.021841,
     "end_time": "2024-05-11T07:34:51.127158",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.105317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.7 Handling Outliers\n",
    "> This section will handle the outliers in the numerical columns.\n",
    "\n",
    "This follows the way we learn in DS module. we are considering 1.5 IQR because it is the value used commonly. Refer https://python.plainenglish.io/identifying-and-handling-outliers-in-pandas-a-step-by-step-guide-fcecd5c6cd3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33444f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:51.177459Z",
     "iopub.status.busy": "2024-05-11T07:34:51.176609Z",
     "iopub.status.idle": "2024-05-11T07:34:51.185286Z",
     "shell.execute_reply": "2024-05-11T07:34:51.184330Z"
    },
    "papermill": {
     "duration": 0.036053,
     "end_time": "2024-05-11T07:34:51.188153",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.152100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(df):\n",
    "    \"\"\"\n",
    "    Handling the outliers by removing the data instance.\n",
    "    \n",
    "    Args :\n",
    "    -df : Pandas dataframe with outliers\n",
    "    \n",
    "    Return:\n",
    "    - pandas.DataFrame : Pandas dataframes with no autliers \n",
    "    \"\"\"\n",
    "    numerics = ['int8','int16', 'int32', 'int64','float16', 'float32', 'float64']\n",
    "    numerical_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in numerics:\n",
    "            numerical_cols.append(col)\n",
    "#     print(numerical_cols)\n",
    "    for col in numerical_cols:\n",
    "        # Calculate 25% percentile and 75% percentile\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "\n",
    "        # Calculate Interquartile Range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        cleaned_df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b93b33",
   "metadata": {
    "papermill": {
     "duration": 0.022118,
     "end_time": "2024-05-11T07:34:51.232134",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.210016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.8 Data Transformations\n",
    "> Data Encodings,and other transformations. Handling nominal data in not possible as onehot encoding is taking too much space. When the nominalcoumn is converted to category type the model accepts it eventhough it is not encoded. Currenly we are handling the ordinal data in the same manner.\n",
    "\n",
    "Can codider ordinal encoding the ordinal attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c117239",
   "metadata": {
    "papermill": {
     "duration": 0.022487,
     "end_time": "2024-05-11T07:34:51.275974",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.253487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e03eb5",
   "metadata": {
    "papermill": {
     "duration": 0.025801,
     "end_time": "2024-05-11T07:34:51.323874",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.298073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.9 Main Function of Data Preprocessing\n",
    "> Main function calls all the preprocessing functionalities for the file specified in filepath and it serialize the preprossesed dataframe while returnning the dataframe.\n",
    "\n",
    "Instead of serializing it is possible to just write the dataframe to a csv file. But it changes our data type back to original data types(int64, float64 and object). Therefore serialization ( **pickle** ) is used. Refer the following stackoverflow https://stackoverflow.com/questions/72610814/why-do-pandas-dataframes-data-types-change-after-exporting-into-a-csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb30a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:51.370753Z",
     "iopub.status.busy": "2024-05-11T07:34:51.369956Z",
     "iopub.status.idle": "2024-05-11T07:34:51.388329Z",
     "shell.execute_reply": "2024-05-11T07:34:51.387374Z"
    },
    "papermill": {
     "duration": 0.044614,
     "end_time": "2024-05-11T07:34:51.390979",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.346365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_preprocess(filepath, test = False, enforce=False, save = True):\n",
    "    \"\"\"\n",
    "    Preprocess the data given the table file path.\n",
    "    \n",
    "    Args:\n",
    "    - filepath: Filepath of the csv file to be preprocessed.\n",
    "    - test: Specify whether the filepath is a test table or not\n",
    "    - enforce: Enforce data preprocessing eventhough a serielized file exists\n",
    "    - save: If True serialize the dataframe\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Preprocessed dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Preprocessing {str(filepath).split('/')[-1]}\", end=\"    \")\n",
    "    \n",
    "    if not enforce and os.path.isfile(f\"/kaggle/working/{str(filepath).split('/')[-1][:-4]}_preprocessed.pkl\"):\n",
    "        end_time = time.time()\n",
    "        print(f\"Finished in {end_time-start_time}\")\n",
    "        return pd.read_pickle(f\"/kaggle/working/{str(filepath).split('/')[-1][:-4]}_preprocessed.pkl\")\n",
    "    \n",
    "    aggregated_df = None\n",
    "    for filepath in glob(str(filepath)):\n",
    "        df = pl.read_csv(filepath, null_values=\"a55475b1\")\n",
    "\n",
    "        if \"num_group2\" in df.columns:\n",
    "            intermediate_aggregated_df = df.group_by([\"case_id\", \"num_group1\"]).agg(Aggregator.get_exprs(df,str(filepath).split('/')[-1]))\n",
    "            aggregated_df = intermediate_aggregated_df.group_by(\"case_id\").agg(Aggregator.get_exprs(intermediate_aggregated_df,str(filepath).split('/')[-1])) if aggregated_df is None else pl.concat([aggregated_df, intermediate_aggregated_df.group_by(\"case_id\").agg(Aggregator.get_exprs(intermediate_aggregated_df,str(filepath).split('/')[-1]))], how=\"diagonal_relaxed\")\n",
    "        elif \"num_group1\" in df.columns:\n",
    "            aggregated_df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df,str(filepath).split('/')[-1])) if aggregated_df is None else pl.concat([aggregated_df, df.group_by(\"case_id\").agg(Aggregator.get_exprs(df,str(filepath).split('/')[-1]))], how=\"diagonal_relaxed\")\n",
    "        else: \n",
    "            aggregated_df = df if aggregated_df is None else pl.concat([aggregated_df, df], how=\"diagonal_relaxed\")\n",
    "\n",
    "    # Remove null dominant columns\n",
    "    if not test:\n",
    "        df = drop_columns_with_high_missing_values(remove_dominated_columns(aggregated_df))\n",
    "    \n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = aggregated_df.to_pandas()\n",
    "    \n",
    "    # Impute missing values\n",
    "    df = impute_missing_values(df)\n",
    "    \n",
    "    # Assign and Optimize the data types\n",
    "    df = handle_data_types(df, pandas=True).sort_values(\"case_id\")\n",
    "    \n",
    "    \n",
    "    if not test:\n",
    "        # Data Reduction\n",
    "        df = reduce_data(df)\n",
    "        # Handling the outliers\n",
    "        # df = handle_outliers(df)\n",
    "    \n",
    "    if save:\n",
    "        df.to_pickle(f\"{str(filepath).split('/')[-1][:-4]}_preprocessed.pkl\")  \n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Finished in {end_time-start_time}\")\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16eb237b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:34:51.500792Z",
     "iopub.status.busy": "2024-05-11T07:34:51.499800Z",
     "iopub.status.idle": "2024-05-11T07:51:08.029073Z",
     "shell.execute_reply": "2024-05-11T07:51:08.027613Z"
    },
    "papermill": {
     "duration": 976.557338,
     "end_time": "2024-05-11T07:51:08.033684",
     "exception": false,
     "start_time": "2024-05-11T07:34:51.476346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train_credit_bureau_a_2_*.csv    Finished in 318.3732600212097\n",
      "Preprocessing train_credit_bureau_a_1_*.csv    Finished in 246.41113305091858\n",
      "Preprocessing train_credit_bureau_b_1.csv    Finished in 2.7569522857666016\n",
      "Preprocessing train_credit_bureau_b_2.csv    Finished in 1.5861260890960693\n",
      "Preprocessing train_applprev_1_*.csv    Finished in 126.11295366287231\n",
      "Preprocessing train_applprev_2.csv    Finished in 17.267749071121216\n",
      "Preprocessing train_debitcard_1.csv    Finished in 1.8593082427978516\n",
      "Preprocessing train_deposit_1.csv    Finished in 3.1188876628875732\n",
      "Preprocessing train_other_1.csv    Finished in 0.15191292762756348\n",
      "Preprocessing train_person_1.csv    Finished in 59.530781507492065\n",
      "Preprocessing train_person_2.csv    Finished in 7.865896224975586\n",
      "Preprocessing train_static_0_*.csv    Finished in 104.5550262928009\n",
      "Preprocessing train_static_cb_*.csv    Finished in 19.471354961395264\n",
      "Preprocessing train_tax_registry_a_1.csv    Finished in 10.240002393722534\n",
      "Preprocessing train_tax_registry_b_1.csv    Finished in 3.475449323654175\n",
      "Preprocessing train_tax_registry_c_1.csv    Finished in 10.709259033203125\n"
     ]
    }
   ],
   "source": [
    "unique_files = [\n",
    "    \"train_credit_bureau_a_2_*.csv\",\n",
    "    \"train_credit_bureau_a_1_*.csv\",\n",
    "    \"train_credit_bureau_b_1.csv\",\n",
    "    \"train_credit_bureau_b_2.csv\",\n",
    "    \"train_applprev_1_*.csv\",\n",
    "    \"train_applprev_2.csv\",\n",
    "    \"train_debitcard_1.csv\",\n",
    "    \"train_deposit_1.csv\",\n",
    "    \"train_other_1.csv\",\n",
    "    \"train_person_1.csv\",\n",
    "    \"train_person_2.csv\",\n",
    "    \"train_static_0_*.csv\",\n",
    "    \"train_static_cb_*.csv\",\n",
    "    \"train_tax_registry_a_1.csv\",\n",
    "    \"train_tax_registry_b_1.csv\",\n",
    "    \"train_tax_registry_c_1.csv\"\n",
    "]\n",
    "\n",
    "base = pl.read_csv(TRAIN_DIR / \"train_base.csv\", null_values=\"a55475b1\")\n",
    "base = base.to_pandas()\n",
    "base = handle_data_types(base, pandas=True)\n",
    "\n",
    "for table in unique_files:\n",
    "    preproceesed_df = data_preprocess(TRAIN_DIR / table)\n",
    "    base = base.merge(preproceesed_df, how=\"left\", on=\"case_id\")\n",
    "    del preproceesed_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "base.to_pickle(f\"final_dataframe.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c396333a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:08.087598Z",
     "iopub.status.busy": "2024-05-11T07:51:08.087179Z",
     "iopub.status.idle": "2024-05-11T07:51:08.731121Z",
     "shell.execute_reply": "2024-05-11T07:51:08.729560Z"
    },
    "papermill": {
     "duration": 0.674537,
     "end_time": "2024-05-11T07:51:08.734693",
     "exception": false,
     "start_time": "2024-05-11T07:51:08.060156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1526659 entries, 0 to 1526658\n",
      "Columns: 344 entries, case_id to tax_registry_c_1.last_num_group1\n",
      "dtypes: category(213), float64(122), int64(9)\n",
      "memory usage: 1.9 GB\n"
     ]
    }
   ],
   "source": [
    "base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b654212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:08.789188Z",
     "iopub.status.busy": "2024-05-11T07:51:08.788746Z",
     "iopub.status.idle": "2024-05-11T07:51:09.412166Z",
     "shell.execute_reply": "2024-05-11T07:51:09.410846Z"
    },
    "papermill": {
     "duration": 0.655252,
     "end_time": "2024-05-11T07:51:09.415540",
     "exception": false,
     "start_time": "2024-05-11T07:51:08.760288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3078099582311805"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.isnull().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6671648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:09.469734Z",
     "iopub.status.busy": "2024-05-11T07:51:09.469295Z",
     "iopub.status.idle": "2024-05-11T07:51:13.755457Z",
     "shell.execute_reply": "2024-05-11T07:51:13.754070Z"
    },
    "papermill": {
     "duration": 4.31516,
     "end_time": "2024-05-11T07:51:13.758126",
     "exception": false,
     "start_time": "2024-05-11T07:51:09.442966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test_credit_bureau_a_2_*.csv    Finished in 0.4523591995239258\n",
      "Preprocessing test_credit_bureau_a_1_*.csv    Finished in 0.30438947677612305\n",
      "Preprocessing test_credit_bureau_b_1.csv    Finished in 0.13110756874084473\n",
      "Preprocessing test_credit_bureau_b_2.csv    Finished in 0.05401325225830078\n",
      "Preprocessing test_applprev_1_*.csv    Finished in 0.15424394607543945\n",
      "Preprocessing test_applprev_2.csv    Finished in 0.03508329391479492\n",
      "Preprocessing test_debitcard_1.csv    Finished in 0.025413990020751953\n",
      "Preprocessing test_deposit_1.csv    Finished in 0.028754234313964844\n",
      "Preprocessing test_other_1.csv    Finished in 0.02477550506591797\n",
      "Preprocessing test_person_1.csv    Finished in 0.1134943962097168\n",
      "Preprocessing test_person_2.csv    Finished in 0.08369874954223633\n",
      "Preprocessing test_static_0_*.csv    Finished in 0.2550642490386963\n",
      "Preprocessing test_static_cb_0.csv    Finished in 0.08885526657104492\n",
      "Preprocessing test_tax_registry_a_1.csv    Finished in 0.023255348205566406\n",
      "Preprocessing test_tax_registry_b_1.csv    Finished in 0.023254871368408203\n",
      "Preprocessing test_tax_registry_c_1.csv    Finished in 0.02616119384765625\n"
     ]
    }
   ],
   "source": [
    "# This section loads the test data set\n",
    "\n",
    "unique_files = [\n",
    "    \"test_credit_bureau_a_2_*.csv\",\n",
    "    \"test_credit_bureau_a_1_*.csv\",\n",
    "    \"test_credit_bureau_b_1.csv\",\n",
    "    \"test_credit_bureau_b_2.csv\",\n",
    "    \"test_applprev_1_*.csv\",\n",
    "    \"test_applprev_2.csv\",\n",
    "    \"test_debitcard_1.csv\",\n",
    "    \"test_deposit_1.csv\",\n",
    "    \"test_other_1.csv\",\n",
    "    \"test_person_1.csv\",\n",
    "    \"test_person_2.csv\",\n",
    "    \"test_static_0_*.csv\",\n",
    "    \"test_static_cb_0.csv\",\n",
    "    \"test_tax_registry_a_1.csv\",\n",
    "    \"test_tax_registry_b_1.csv\",\n",
    "    \"test_tax_registry_c_1.csv\"\n",
    "]\n",
    "\n",
    "submission_base = pl.read_csv(TEST_DIR / \"test_base.csv\", null_values=\"a55475b1\")\n",
    "submission_base = submission_base.to_pandas()\n",
    "submission_base = handle_data_types(submission_base, pandas=True)\n",
    "\n",
    "for table in unique_files:\n",
    "    preproceesed_df = data_preprocess(TEST_DIR / table, test=True, enforce=True)\n",
    "    submission_base = submission_base.merge(preproceesed_df, how=\"left\", on=\"case_id\")\n",
    "    del preproceesed_df\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "submission_base.to_pickle(f\"submission_dataframe.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a9948f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:13.977466Z",
     "iopub.status.busy": "2024-05-11T07:51:13.976335Z",
     "iopub.status.idle": "2024-05-11T07:51:14.174673Z",
     "shell.execute_reply": "2024-05-11T07:51:14.173483Z"
    },
    "papermill": {
     "duration": 0.301568,
     "end_time": "2024-05-11T07:51:14.177242",
     "exception": false,
     "start_time": "2024-05-11T07:51:13.875674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Columns: 1098 entries, case_id to tax_registry_c_1.last_num_group1\n",
      "dtypes: category(633), datetime64[ns](60), float64(386), int64(19)\n",
      "memory usage: 140.6 KB\n"
     ]
    }
   ],
   "source": [
    "submission_base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14720f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:14.346864Z",
     "iopub.status.busy": "2024-05-11T07:51:14.345769Z",
     "iopub.status.idle": "2024-05-11T07:51:14.407359Z",
     "shell.execute_reply": "2024-05-11T07:51:14.406353Z"
    },
    "papermill": {
     "duration": 0.162298,
     "end_time": "2024-05-11T07:51:14.411098",
     "exception": false,
     "start_time": "2024-05-11T07:51:14.248800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5724043715846995"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_base.isnull().mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae3be7",
   "metadata": {
    "papermill": {
     "duration": 0.092571,
     "end_time": "2024-05-11T07:51:14.538016",
     "exception": false,
     "start_time": "2024-05-11T07:51:14.445445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe145e",
   "metadata": {
    "papermill": {
     "duration": 0.091022,
     "end_time": "2024-05-11T07:51:14.720088",
     "exception": false,
     "start_time": "2024-05-11T07:51:14.629066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Handling Null Features\n",
    "\n",
    "> Currently not handling the null values as null values are accepted by the model.\n",
    "\n",
    "Can drop, impute or user a **ML model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c259595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:14.911517Z",
     "iopub.status.busy": "2024-05-11T07:51:14.910950Z",
     "iopub.status.idle": "2024-05-11T07:51:16.626852Z",
     "shell.execute_reply": "2024-05-11T07:51:16.625280Z"
    },
    "papermill": {
     "duration": 1.809254,
     "end_time": "2024-05-11T07:51:16.630444",
     "exception": false,
     "start_time": "2024-05-11T07:51:14.821190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07485520473137748"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = drop_columns_with_high_missing_values(base)\n",
    "df.isnull().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2eab79a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:16.688425Z",
     "iopub.status.busy": "2024-05-11T07:51:16.687875Z",
     "iopub.status.idle": "2024-05-11T07:51:21.767222Z",
     "shell.execute_reply": "2024-05-11T07:51:21.765977Z"
    },
    "papermill": {
     "duration": 5.112535,
     "end_time": "2024-05-11T07:51:21.770874",
     "exception": false,
     "start_time": "2024-05-11T07:51:16.658339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute_missing_values(df)\n",
    "df.isnull().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d20b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:21.827799Z",
     "iopub.status.busy": "2024-05-11T07:51:21.827298Z",
     "iopub.status.idle": "2024-05-11T07:51:21.942441Z",
     "shell.execute_reply": "2024-05-11T07:51:21.940580Z"
    },
    "papermill": {
     "duration": 0.148,
     "end_time": "2024-05-11T07:51:21.946431",
     "exception": false,
     "start_time": "2024-05-11T07:51:21.798431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = list(df.select_dtypes(\"category\").columns)\n",
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "590d8628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:22.002988Z",
     "iopub.status.busy": "2024-05-11T07:51:22.002547Z",
     "iopub.status.idle": "2024-05-11T07:51:54.873595Z",
     "shell.execute_reply": "2024-05-11T07:51:54.871894Z"
    },
    "papermill": {
     "duration": 32.927286,
     "end_time": "2024-05-11T07:51:54.902030",
     "exception": false,
     "start_time": "2024-05-11T07:51:21.974744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1526659 entries, 0 to 1526658\n",
      "Columns: 230 entries, case_id to thirdquarter_1082L\n",
      "dtypes: category(161), float64(64), int64(5)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your pandas DataFrame and cat_cols is the list of categorical columns\n",
    "\n",
    "# Step 1: Calculate correlation matrix\n",
    "\n",
    "\n",
    "# Step 2: Mask out correlations involving categorical features\n",
    "numerical_cols = [col for col in df.columns if col not in cat_cols and col not in ['WEEK_NUM', 'target', 'case_id']]\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "corr_matrix = corr_matrix.loc[numerical_cols, numerical_cols]\n",
    "\n",
    "# Step 3: Identify highly correlated features\n",
    "highly_correlated = (corr_matrix.abs() > 0.8) & (corr_matrix.abs() < 1.0)\n",
    "correlated_features = set()\n",
    "for col in highly_correlated.columns:\n",
    "    correlated_features.update(set(highly_correlated.index[highly_correlated[col]]))\n",
    "correlated_features = list(correlated_features)\n",
    "\n",
    "# Step 4: Remove highly correlated features from the DataFrame\n",
    "df_filtered = df.drop(columns=correlated_features)\n",
    "\n",
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab6cf8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:54.959136Z",
     "iopub.status.busy": "2024-05-11T07:51:54.958748Z",
     "iopub.status.idle": "2024-05-11T07:51:54.967961Z",
     "shell.execute_reply": "2024-05-11T07:51:54.966678Z"
    },
    "papermill": {
     "duration": 0.04208,
     "end_time": "2024-05-11T07:51:54.970357",
     "exception": false,
     "start_time": "2024-05-11T07:51:54.928277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "789b243f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:55.026018Z",
     "iopub.status.busy": "2024-05-11T07:51:55.025471Z",
     "iopub.status.idle": "2024-05-11T07:51:55.070235Z",
     "shell.execute_reply": "2024-05-11T07:51:55.068682Z"
    },
    "papermill": {
     "duration": 0.075988,
     "end_time": "2024-05-11T07:51:55.072955",
     "exception": false,
     "start_time": "2024-05-11T07:51:54.996967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Columns: 229 entries, case_id to thirdquarter_1082L\n",
      "dtypes: category(161), float64(65), int64(3)\n",
      "memory usage: 44.6 KB\n"
     ]
    }
   ],
   "source": [
    "currently_selected_columns = list(df.columns)\n",
    "currently_selected_columns.remove(\"target\")\n",
    "submission_base = submission_base[currently_selected_columns]\n",
    "submission_base.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fce29010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:55.130809Z",
     "iopub.status.busy": "2024-05-11T07:51:55.129727Z",
     "iopub.status.idle": "2024-05-11T07:51:55.156403Z",
     "shell.execute_reply": "2024-05-11T07:51:55.154801Z"
    },
    "papermill": {
     "duration": 0.058894,
     "end_time": "2024-05-11T07:51:55.160214",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.101320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3707423580786026"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_base.isnull().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b656804e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:55.219331Z",
     "iopub.status.busy": "2024-05-11T07:51:55.218917Z",
     "iopub.status.idle": "2024-05-11T07:51:55.226689Z",
     "shell.execute_reply": "2024-05-11T07:51:55.225460Z"
    },
    "papermill": {
     "duration": 0.039926,
     "end_time": "2024-05-11T07:51:55.229434",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.189508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = submission_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28440518",
   "metadata": {
    "papermill": {
     "duration": 0.027596,
     "end_time": "2024-05-11T07:51:55.285634",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.258038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Handling Redundent Features\n",
    "> Not handling it yet. Can redo the things we did during data loading section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa7c15",
   "metadata": {
    "papermill": {
     "duration": 0.027958,
     "end_time": "2024-05-11T07:51:55.348301",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.320343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909668f0",
   "metadata": {
    "papermill": {
     "duration": 0.027003,
     "end_time": "2024-05-11T07:51:55.401996",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.374993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Feature Selection\n",
    "> Currently doing nothing. Can use cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a127fa",
   "metadata": {
    "papermill": {
     "duration": 0.026599,
     "end_time": "2024-05-11T07:51:55.457386",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.430787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef320a84",
   "metadata": {
    "papermill": {
     "duration": 0.029104,
     "end_time": "2024-05-11T07:51:55.513381",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.484277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaaf322",
   "metadata": {
    "papermill": {
     "duration": 0.02645,
     "end_time": "2024-05-11T07:51:55.567289",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.540839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1 Handling Class Imbalance*\n",
    "> Currently not handling\n",
    "\n",
    "Can try out SMOTE or balancing by removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b342ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:55.625657Z",
     "iopub.status.busy": "2024-05-11T07:51:55.625257Z",
     "iopub.status.idle": "2024-05-11T07:51:57.276476Z",
     "shell.execute_reply": "2024-05-11T07:51:57.275176Z"
    },
    "papermill": {
     "duration": 1.683797,
     "end_time": "2024-05-11T07:51:57.279778",
     "exception": false,
     "start_time": "2024-05-11T07:51:55.595981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "      <th>max_mean_pmts_dpd_303P</th>\n",
       "      <th>max_mean_pmts_overdue_1152A</th>\n",
       "      <th>last_max_pmts_dpd_1073P</th>\n",
       "      <th>last_max_pmts_dpd_303P</th>\n",
       "      <th>last_max_pmts_overdue_1140A</th>\n",
       "      <th>last_max_pmts_overdue_1152A</th>\n",
       "      <th>...</th>\n",
       "      <th>days180_256L</th>\n",
       "      <th>days30_165L</th>\n",
       "      <th>days360_512L</th>\n",
       "      <th>days90_310L</th>\n",
       "      <th>firstquarter_103L</th>\n",
       "      <th>fourthquarter_440L</th>\n",
       "      <th>maritalst_385M</th>\n",
       "      <th>numberofqueries_373L</th>\n",
       "      <th>secondquarter_766L</th>\n",
       "      <th>thirdquarter_1082L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223173</th>\n",
       "      <td>607706</td>\n",
       "      <td>201901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99.456545</td>\n",
       "      <td>7503.658123</td>\n",
       "      <td>9.56664</td>\n",
       "      <td>48.231823</td>\n",
       "      <td>1414.021089</td>\n",
       "      <td>3639.015467</td>\n",
       "      <td>...</td>\n",
       "      <td>2.388656</td>\n",
       "      <td>0.517708</td>\n",
       "      <td>4.777066</td>\n",
       "      <td>1.21142</td>\n",
       "      <td>2.86059</td>\n",
       "      <td>2.851214</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>4.777066</td>\n",
       "      <td>2.688482</td>\n",
       "      <td>2.918342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829869</th>\n",
       "      <td>1423898</td>\n",
       "      <td>201906</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11998.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616358</th>\n",
       "      <td>1000891</td>\n",
       "      <td>202007</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>99.456545</td>\n",
       "      <td>7503.658123</td>\n",
       "      <td>9.56664</td>\n",
       "      <td>48.231823</td>\n",
       "      <td>1414.021089</td>\n",
       "      <td>3639.015467</td>\n",
       "      <td>...</td>\n",
       "      <td>2.388656</td>\n",
       "      <td>0.517708</td>\n",
       "      <td>4.777066</td>\n",
       "      <td>1.21142</td>\n",
       "      <td>2.86059</td>\n",
       "      <td>2.851214</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>4.777066</td>\n",
       "      <td>2.688482</td>\n",
       "      <td>2.918342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525389</th>\n",
       "      <td>909922</td>\n",
       "      <td>201912</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994980</th>\n",
       "      <td>1589009</td>\n",
       "      <td>201910</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>6.727273</td>\n",
       "      <td>288.028571</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115157</th>\n",
       "      <td>1709186</td>\n",
       "      <td>201912</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>3.384615</td>\n",
       "      <td>1801.338492</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194137</th>\n",
       "      <td>239571</td>\n",
       "      <td>202007</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>225.483337</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2714.400100</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961998</th>\n",
       "      <td>1556027</td>\n",
       "      <td>201909</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>30.615385</td>\n",
       "      <td>1406.276931</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5393.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398477</th>\n",
       "      <td>783010</td>\n",
       "      <td>201908</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>786.058824</td>\n",
       "      <td>4901.272471</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1251.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5219.026400</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294342</th>\n",
       "      <td>1888371</td>\n",
       "      <td>202008</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>1027.083333</td>\n",
       "      <td>87845.361167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7913.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3439d993</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95988 rows Ã— 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_id   MONTH  WEEK_NUM  target  max_mean_pmts_dpd_303P  \\\n",
       "223173    607706  201901         1       1               99.456545   \n",
       "829869   1423898  201906        25       0                0.000000   \n",
       "616358   1000891  202007        80       1               99.456545   \n",
       "525389    909922  201912        51       0                0.000000   \n",
       "994980   1589009  201910        41       0                6.727273   \n",
       "...          ...     ...       ...     ...                     ...   \n",
       "1115157  1709186  201912        51       0                3.384615   \n",
       "194137    239571  202007        81       1                0.083333   \n",
       "961998   1556027  201909        38       1               30.615385   \n",
       "398477    783010  201908        34       0              786.058824   \n",
       "1294342  1888371  202008        83       0             1027.083333   \n",
       "\n",
       "         max_mean_pmts_overdue_1152A  last_max_pmts_dpd_1073P  \\\n",
       "223173                   7503.658123                  9.56664   \n",
       "829869                      0.000000                  0.00000   \n",
       "616358                   7503.658123                  9.56664   \n",
       "525389                      0.000000                  0.00000   \n",
       "994980                    288.028571                  0.00000   \n",
       "...                              ...                      ...   \n",
       "1115157                  1801.338492                  0.00000   \n",
       "194137                    225.483337                  0.00000   \n",
       "961998                   1406.276931                  0.00000   \n",
       "398477                   4901.272471                  0.00000   \n",
       "1294342                 87845.361167                  0.00000   \n",
       "\n",
       "         last_max_pmts_dpd_303P  last_max_pmts_overdue_1140A  \\\n",
       "223173                48.231823                  1414.021089   \n",
       "829869                 3.000000                     0.000000   \n",
       "616358                48.231823                  1414.021089   \n",
       "525389                 0.000000                     0.000000   \n",
       "994980                 2.000000                     0.000000   \n",
       "...                         ...                          ...   \n",
       "1115157                0.000000                     0.000000   \n",
       "194137                 1.000000                     0.000000   \n",
       "961998               110.000000                     0.000000   \n",
       "398477              1251.000000                     0.000000   \n",
       "1294342                4.000000                     0.000000   \n",
       "\n",
       "         last_max_pmts_overdue_1152A  ...  days180_256L days30_165L  \\\n",
       "223173                   3639.015467  ...      2.388656    0.517708   \n",
       "829869                  11998.400000  ...      2.000000    0.000000   \n",
       "616358                   3639.015467  ...      2.388656    0.517708   \n",
       "525389                      0.000000  ...      1.000000    1.000000   \n",
       "994980                   2016.400000  ...      0.000000    0.000000   \n",
       "...                              ...  ...           ...         ...   \n",
       "1115157                     0.000000  ...      1.000000    0.000000   \n",
       "194137                   2714.400100  ...      5.000000    2.000000   \n",
       "961998                   5393.000000  ...      4.000000    3.000000   \n",
       "398477                   5219.026400  ...      1.000000    1.000000   \n",
       "1294342                  7913.600000  ...      1.000000    0.000000   \n",
       "\n",
       "        days360_512L days90_310L firstquarter_103L fourthquarter_440L  \\\n",
       "223173      4.777066     1.21142           2.86059           2.851214   \n",
       "829869      4.000000     0.00000           4.00000           1.000000   \n",
       "616358      4.777066     1.21142           2.86059           2.851214   \n",
       "525389      2.000000     1.00000           2.00000           2.000000   \n",
       "994980      3.000000     0.00000           1.00000           6.000000   \n",
       "...              ...         ...               ...                ...   \n",
       "1115157     3.000000     1.00000           2.00000           3.000000   \n",
       "194137      8.000000     4.00000           3.00000           2.000000   \n",
       "961998      5.000000     3.00000           0.00000           1.000000   \n",
       "398477      1.000000     1.00000           0.00000           0.000000   \n",
       "1294342     2.000000     0.00000           2.00000           2.000000   \n",
       "\n",
       "        maritalst_385M numberofqueries_373L secondquarter_766L  \\\n",
       "223173        3439d993             4.777066           2.688482   \n",
       "829869        3439d993             4.000000           3.000000   \n",
       "616358        3439d993             4.777066           2.688482   \n",
       "525389        3439d993             2.000000           3.000000   \n",
       "994980        3439d993             3.000000           0.000000   \n",
       "...                ...                  ...                ...   \n",
       "1115157       3439d993             3.000000           0.000000   \n",
       "194137        3439d993             8.000000           2.000000   \n",
       "961998        3439d993             5.000000           1.000000   \n",
       "398477        3439d993             1.000000           1.000000   \n",
       "1294342       3439d993             2.000000           0.000000   \n",
       "\n",
       "        thirdquarter_1082L  \n",
       "223173            2.918342  \n",
       "829869            2.000000  \n",
       "616358            2.918342  \n",
       "525389            4.000000  \n",
       "994980            3.000000  \n",
       "...                    ...  \n",
       "1115157           0.000000  \n",
       "194137            4.000000  \n",
       "961998            3.000000  \n",
       "398477            1.000000  \n",
       "1294342           0.000000  \n",
       "\n",
       "[95988 rows x 230 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = df[df['target'] == 0]\n",
    "minority_class = df[df['target'] == 1]\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                 replace=False,     # Sample without replacement\n",
    "                                 n_samples=len(minority_class),    # Match minority class size\n",
    "                                 random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_downsampled = df_downsampled.sample(frac=1, random_state=42)\n",
    "\n",
    "# Now df_downsampled contains a balanced dataset\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b60fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:57.344103Z",
     "iopub.status.busy": "2024-05-11T07:51:57.343290Z",
     "iopub.status.idle": "2024-05-11T07:51:57.540365Z",
     "shell.execute_reply": "2024-05-11T07:51:57.539147Z"
    },
    "papermill": {
     "duration": 0.233065,
     "end_time": "2024-05-11T07:51:57.543535",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.310470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 95988 entries, 223173 to 1294342\n",
      "Columns: 230 entries, case_id to thirdquarter_1082L\n",
      "dtypes: category(161), float64(64), int64(5)\n",
      "memory usage: 77.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = df_downsampled\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf99563",
   "metadata": {
    "papermill": {
     "duration": 0.029955,
     "end_time": "2024-05-11T07:51:57.601326",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.571371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2 Splitting Data\n",
    "\n",
    "> Train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d79c856b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:57.661742Z",
     "iopub.status.busy": "2024-05-11T07:51:57.661081Z",
     "iopub.status.idle": "2024-05-11T07:51:57.667206Z",
     "shell.execute_reply": "2024-05-11T07:51:57.665859Z"
    },
    "papermill": {
     "duration": 0.040043,
     "end_time": "2024-05-11T07:51:57.670450",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.630407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(data.copy().drop([\"target\"], axis = 1), data[\"target\"], test_size=0.4, random_state=42)\n",
    "# X_valid, X_test, y_valid, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640609c1",
   "metadata": {
    "papermill": {
     "duration": 0.028693,
     "end_time": "2024-05-11T07:51:57.728458",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.699765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3 Setting Model Parameters*\n",
    "\n",
    "> Can use cross validation with gridsearch or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10110fa",
   "metadata": {
    "papermill": {
     "duration": 0.027933,
     "end_time": "2024-05-11T07:51:57.785226",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.757293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e41f16",
   "metadata": {
    "papermill": {
     "duration": 0.027867,
     "end_time": "2024-05-11T07:51:57.841781",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.813914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.4 Training the Best Model*\n",
    "\n",
    "> Should maximize the AUC\n",
    "\n",
    "There can be more models that provide better results. And also wecan consider model stacking and boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1108e2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:57.900406Z",
     "iopub.status.busy": "2024-05-11T07:51:57.899659Z",
     "iopub.status.idle": "2024-05-11T07:51:58.011859Z",
     "shell.execute_reply": "2024-05-11T07:51:58.009789Z"
    },
    "papermill": {
     "duration": 0.14559,
     "end_time": "2024-05-11T07:51:58.014987",
     "exception": false,
     "start_time": "2024-05-11T07:51:57.869397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (67191, 230)\n",
      "Testing set shape: (28797, 230)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 70% training and 30% testing\n",
    "X_train, X_test = train_test_split(df_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Optionally, you can print the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "df_train = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06a4e735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:58.076421Z",
     "iopub.status.busy": "2024-05-11T07:51:58.075616Z",
     "iopub.status.idle": "2024-05-11T07:51:58.151039Z",
     "shell.execute_reply": "2024-05-11T07:51:58.148896Z"
    },
    "papermill": {
     "duration": 0.110278,
     "end_time": "2024-05-11T07:51:58.154910",
     "exception": false,
     "start_time": "2024-05-11T07:51:58.044632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "815b9687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:58.214259Z",
     "iopub.status.busy": "2024-05-11T07:51:58.213821Z",
     "iopub.status.idle": "2024-05-11T07:51:58.225536Z",
     "shell.execute_reply": "2024-05-11T07:51:58.223576Z"
    },
    "papermill": {
     "duration": 0.045791,
     "end_time": "2024-05-11T07:51:58.228810",
     "exception": false,
     "start_time": "2024-05-11T07:51:58.183019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    33658\n",
       "1    33533\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d769cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:51:58.290963Z",
     "iopub.status.busy": "2024-05-11T07:51:58.289173Z",
     "iopub.status.idle": "2024-05-11T07:52:01.658735Z",
     "shell.execute_reply": "2024-05-11T07:52:01.656663Z"
    },
    "papermill": {
     "duration": 3.403667,
     "end_time": "2024-05-11T07:52:01.661616",
     "exception": false,
     "start_time": "2024-05-11T07:51:58.257949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train[cat_cols] = df_train[cat_cols].astype(str)\n",
    "df_test[cat_cols] = df_test[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81f4be20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:52:01.722509Z",
     "iopub.status.busy": "2024-05-11T07:52:01.722069Z",
     "iopub.status.idle": "2024-05-11T07:52:01.746399Z",
     "shell.execute_reply": "2024-05-11T07:52:01.744561Z"
    },
    "papermill": {
     "duration": 0.058374,
     "end_time": "2024-05-11T07:52:01.748915",
     "exception": false,
     "start_time": "2024-05-11T07:52:01.690541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Columns: 229 entries, case_id to thirdquarter_1082L\n",
      "dtypes: float64(65), int64(3), object(161)\n",
      "memory usage: 18.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18be8590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:52:01.810053Z",
     "iopub.status.busy": "2024-05-11T07:52:01.809631Z",
     "iopub.status.idle": "2024-05-11T07:52:01.817041Z",
     "shell.execute_reply": "2024-05-11T07:52:01.815385Z"
    },
    "papermill": {
     "duration": 0.040781,
     "end_time": "2024-05-11T07:52:01.819674",
     "exception": false,
     "start_time": "2024-05-11T07:52:01.778893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e751769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:52:01.878393Z",
     "iopub.status.busy": "2024-05-11T07:52:01.877929Z",
     "iopub.status.idle": "2024-05-11T07:52:01.887117Z",
     "shell.execute_reply": "2024-05-11T07:52:01.885837Z"
    },
    "papermill": {
     "duration": 0.041461,
     "end_time": "2024-05-11T07:52:01.889698",
     "exception": false,
     "start_time": "2024-05-11T07:52:01.848237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test(y_test, y_pred_binary):\n",
    "\n",
    "    # actual values\n",
    "    actual = y_test\n",
    "    # predicted values\n",
    "    predicted = y_pred_binary\n",
    "\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "    print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "    # outcome values order in sklearn\n",
    "    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "    print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "    # classification report for precision, recall f1-score and accuracy\n",
    "    matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "    print('Classification reportÂ :Â \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ea5502c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T07:52:01.950269Z",
     "iopub.status.busy": "2024-05-11T07:52:01.949837Z",
     "iopub.status.idle": "2024-05-11T10:01:16.533916Z",
     "shell.execute_reply": "2024-05-11T10:01:16.531937Z"
    },
    "papermill": {
     "duration": 7754.618779,
     "end_time": "2024-05-11T10:01:16.537747",
     "exception": false,
     "start_time": "2024-05-11T07:52:01.918968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7205481\tbest: 0.7205481 (0)\ttotal: 1.64s\tremaining: 32m 44s\n",
      "300:\ttest: 0.8287195\tbest: 0.8287195 (300)\ttotal: 6m 17s\tremaining: 18m 47s\n",
      "600:\ttest: 0.8362330\tbest: 0.8362330 (600)\ttotal: 12m 48s\tremaining: 12m 46s\n",
      "900:\ttest: 0.8390473\tbest: 0.8390473 (900)\ttotal: 19m 17s\tremaining: 6m 24s\n",
      "1199:\ttest: 0.8404054\tbest: 0.8404054 (1199)\ttotal: 25m 24s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8404054285\n",
      "bestIteration = 1199\n",
      "\n",
      "Confusion matrix : \n",
      " [[5040 1634]\n",
      " [1597 5014]]\n",
      "Outcome values : \n",
      " 5040 1634 1597 5014\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.76      0.76      6674\n",
      "           0       0.75      0.76      0.76      6611\n",
      "\n",
      "    accuracy                           0.76     13285\n",
      "   macro avg       0.76      0.76      0.76     13285\n",
      "weighted avg       0.76      0.76      0.76     13285\n",
      "\n",
      "Accuracy: 0.7567933759879564\n",
      "mae score:  0.33618672129100835\n",
      "rmse score:  0.40384579378242474\n",
      "r2 score:  0.3476196284056833\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.83167\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid_0's auc: 0.831946\n",
      "Confusion matrix : \n",
      " [[5008 1666]\n",
      " [1616 4995]]\n",
      "Outcome values : \n",
      " 5008 1666 1616 4995\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.75      0.75      6674\n",
      "           0       0.75      0.76      0.75      6611\n",
      "\n",
      "    accuracy                           0.75     13285\n",
      "   macro avg       0.75      0.75      0.75     13285\n",
      "weighted avg       0.75      0.75      0.75     13285\n",
      "\n",
      "Accuracy: 0.7529544599171999\n",
      "mae score:  0.3429217321378577\n",
      "rmse score:  0.4090583166634165\n",
      "r2 score:  0.3306701221359071\n",
      "0:\ttest: 0.7052785\tbest: 0.7052785 (0)\ttotal: 1.35s\tremaining: 26m 55s\n",
      "300:\ttest: 0.8257841\tbest: 0.8257841 (300)\ttotal: 6m 27s\tremaining: 19m 15s\n",
      "600:\ttest: 0.8337610\tbest: 0.8337610 (600)\ttotal: 12m 32s\tremaining: 12m 29s\n",
      "900:\ttest: 0.8367222\tbest: 0.8367222 (900)\ttotal: 18m 49s\tremaining: 6m 14s\n",
      "1199:\ttest: 0.8382093\tbest: 0.8382093 (1199)\ttotal: 25m 10s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8382093358\n",
      "bestIteration = 1199\n",
      "\n",
      "Confusion matrix : \n",
      " [[5157 1579]\n",
      " [1784 5353]]\n",
      "Outcome values : \n",
      " 5157 1579 1784 5353\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.77      0.75      6736\n",
      "           0       0.77      0.75      0.76      7137\n",
      "\n",
      "    accuracy                           0.76     13873\n",
      "   macro avg       0.76      0.76      0.76     13873\n",
      "weighted avg       0.76      0.76      0.76     13873\n",
      "\n",
      "Accuracy: 0.7575866791609601\n",
      "mae score:  0.33792490999425995\n",
      "rmse score:  0.40524385503810856\n",
      "r2 score:  0.34256037909160775\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.829034\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's auc: 0.829418\n",
      "Confusion matrix : \n",
      " [[5147 1589]\n",
      " [1868 5269]]\n",
      "Outcome values : \n",
      " 5147 1589 1868 5269\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.76      0.75      6736\n",
      "           0       0.77      0.74      0.75      7137\n",
      "\n",
      "    accuracy                           0.75     13873\n",
      "   macro avg       0.75      0.75      0.75     13873\n",
      "weighted avg       0.75      0.75      0.75     13873\n",
      "\n",
      "Accuracy: 0.7508109277012903\n",
      "mae score:  0.34391696333325544\n",
      "rmse score:  0.4104118687661871\n",
      "r2 score:  0.32568499975084597\n",
      "0:\ttest: 0.7214891\tbest: 0.7214891 (0)\ttotal: 1.29s\tremaining: 25m 42s\n",
      "300:\ttest: 0.8309924\tbest: 0.8309924 (300)\ttotal: 6m 18s\tremaining: 18m 49s\n",
      "600:\ttest: 0.8375485\tbest: 0.8375485 (600)\ttotal: 12m 27s\tremaining: 12m 25s\n",
      "900:\ttest: 0.8398113\tbest: 0.8398113 (900)\ttotal: 18m 38s\tremaining: 6m 11s\n",
      "1199:\ttest: 0.8407771\tbest: 0.8407857 (1193)\ttotal: 24m 48s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8407857193\n",
      "bestIteration = 1193\n",
      "\n",
      "Shrink model to first 1194 iterations.\n",
      "Confusion matrix : \n",
      " [[5509 1488]\n",
      " [1819 5080]]\n",
      "Outcome values : \n",
      " 5509 1488 1819 5080\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.79      0.77      6997\n",
      "           0       0.77      0.74      0.75      6899\n",
      "\n",
      "    accuracy                           0.76     13896\n",
      "   macro avg       0.76      0.76      0.76     13896\n",
      "weighted avg       0.76      0.76      0.76     13896\n",
      "\n",
      "Accuracy: 0.7620178468624065\n",
      "mae score:  0.33123384134508205\n",
      "rmse score:  0.4028541082652325\n",
      "r2 score:  0.3508019811772727\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.831363\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's auc: 0.8318\n",
      "Confusion matrix : \n",
      " [[5483 1514]\n",
      " [1912 4987]]\n",
      "Outcome values : \n",
      " 5483 1514 1912 4987\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.78      0.76      6997\n",
      "           0       0.77      0.72      0.74      6899\n",
      "\n",
      "    accuracy                           0.75     13896\n",
      "   macro avg       0.75      0.75      0.75     13896\n",
      "weighted avg       0.75      0.75      0.75     13896\n",
      "\n",
      "Accuracy: 0.7534542314335061\n",
      "mae score:  0.3420517997465486\n",
      "rmse score:  0.40841450149212966\n",
      "r2 score:  0.332757193766685\n",
      "0:\ttest: 0.7158473\tbest: 0.7158473 (0)\ttotal: 1.36s\tremaining: 27m 14s\n",
      "300:\ttest: 0.8234699\tbest: 0.8234699 (300)\ttotal: 6m 26s\tremaining: 19m 15s\n",
      "600:\ttest: 0.8308102\tbest: 0.8308102 (600)\ttotal: 12m 37s\tremaining: 12m 35s\n",
      "900:\ttest: 0.8335359\tbest: 0.8335359 (900)\ttotal: 18m 48s\tremaining: 6m 14s\n",
      "1199:\ttest: 0.8350333\tbest: 0.8350450 (1198)\ttotal: 25m 1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8350449567\n",
      "bestIteration = 1198\n",
      "\n",
      "Shrink model to first 1199 iterations.\n",
      "Confusion matrix : \n",
      " [[5121 1511]\n",
      " [1659 4676]]\n",
      "Outcome values : \n",
      " 5121 1511 1659 4676\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.77      0.76      6632\n",
      "           0       0.76      0.74      0.75      6335\n",
      "\n",
      "    accuracy                           0.76     12967\n",
      "   macro avg       0.76      0.76      0.76     12967\n",
      "weighted avg       0.76      0.76      0.76     12967\n",
      "\n",
      "Accuracy: 0.7555332767795172\n",
      "mae score:  0.3351945909864629\n",
      "rmse score:  0.40632717153334147\n",
      "r2 score:  0.3392462828326037\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.827303\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid_0's auc: 0.827343\n",
      "Confusion matrix : \n",
      " [[5135 1497]\n",
      " [1746 4589]]\n",
      "Outcome values : \n",
      " 5135 1497 1746 4589\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.77      0.76      6632\n",
      "           0       0.75      0.72      0.74      6335\n",
      "\n",
      "    accuracy                           0.75     12967\n",
      "   macro avg       0.75      0.75      0.75     12967\n",
      "weighted avg       0.75      0.75      0.75     12967\n",
      "\n",
      "Accuracy: 0.7499036014498341\n",
      "mae score:  0.3472033260279956\n",
      "rmse score:  0.4112021545148171\n",
      "r2 score:  0.32329614907935367\n",
      "0:\ttest: 0.7101135\tbest: 0.7101135 (0)\ttotal: 1.33s\tremaining: 26m 32s\n",
      "300:\ttest: 0.8237006\tbest: 0.8237006 (300)\ttotal: 6m 15s\tremaining: 18m 40s\n",
      "600:\ttest: 0.8317525\tbest: 0.8317525 (600)\ttotal: 12m 24s\tremaining: 12m 22s\n",
      "900:\ttest: 0.8348056\tbest: 0.8348056 (900)\ttotal: 18m 36s\tremaining: 6m 10s\n",
      "1199:\ttest: 0.8365259\tbest: 0.8365359 (1198)\ttotal: 24m 51s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.836535864\n",
      "bestIteration = 1198\n",
      "\n",
      "Shrink model to first 1199 iterations.\n",
      "Confusion matrix : \n",
      " [[5086 1408]\n",
      " [1743 4933]]\n",
      "Outcome values : \n",
      " 5086 1408 1743 4933\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.78      0.76      6494\n",
      "           0       0.78      0.74      0.76      6676\n",
      "\n",
      "    accuracy                           0.76     13170\n",
      "   macro avg       0.76      0.76      0.76     13170\n",
      "weighted avg       0.76      0.76      0.76     13170\n",
      "\n",
      "Accuracy: 0.7607441154138193\n",
      "mae score:  0.3333539715322783\n",
      "rmse score:  0.4057419719708451\n",
      "r2 score:  0.34136802801686206\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.827231\n",
      "[400]\tvalid_0's auc: 0.827596\n",
      "Early stopping, best iteration is:\n",
      "[303]\tvalid_0's auc: 0.828228\n",
      "Confusion matrix : \n",
      " [[5041 1453]\n",
      " [1810 4866]]\n",
      "Outcome values : \n",
      " 5041 1453 1810 4866\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.78      0.76      6494\n",
      "           0       0.77      0.73      0.75      6676\n",
      "\n",
      "    accuracy                           0.75     13170\n",
      "   macro avg       0.75      0.75      0.75     13170\n",
      "weighted avg       0.75      0.75      0.75     13170\n",
      "\n",
      "Accuracy: 0.7522399392558846\n",
      "mae score:  0.3386775549267354\n",
      "rmse score:  0.41076270668107506\n",
      "r2 score:  0.3249670823597446\n",
      "CV AUC scores:  [0.8404047258800376, 0.8382106878709426, 0.8407848285175041, 0.8350440522762564, 0.8365307202500423]\n",
      "Maximum CV AUC score:  0.8407848285175041\n",
      "CV AUC scores:  [0.8319455768523025, 0.8294184158563466, 0.8317996139525392, 0.8273428299136567, 0.8282279692938663]\n",
      "Maximum CV AUC score:  0.8319455768523025\n",
      "CPU times: user 7h 49min 14s, sys: 12min 51s, total: 8h 2min 5s\n",
      "Wall time: 2h 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "\n",
    "fitted_models_cat = []\n",
    "fitted_models_lgb = []\n",
    "\n",
    "cv_scores_cat = []\n",
    "cv_scores_lgb = []\n",
    "\n",
    "\n",
    "for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n",
    "    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n",
    "    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n",
    "    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n",
    "    clf = CatBoostClassifier(\n",
    "    eval_metric='AUC',\n",
    "    learning_rate=0.03,\n",
    "    iterations=1200)\n",
    "    random_seed=3107\n",
    "    clf.fit(train_pool, eval_set=val_pool,verbose=300)\n",
    "    fitted_models_cat.append(clf)\n",
    "    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_cat.append(auc_score)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_valid]\n",
    "    test(y_valid, y_pred_binary)\n",
    "    accuracy = accuracy_score(y_valid, y_pred_binary)\n",
    "    mae_score_cat = mean_absolute_error(y_valid, y_pred_valid)\n",
    "    rmse_score_cat = mean_squared_error(y_valid, y_pred_valid, squared=False)\n",
    "    r2_score_cat = r2_score(y_valid, y_pred_valid)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"mae score: \", mae_score_cat)\n",
    "    print(\"rmse score: \", rmse_score_cat)\n",
    "    print(\"r2 score: \", r2_score_cat)\n",
    "    \n",
    "    \n",
    "    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
    "    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_valid, y_valid)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "    \n",
    "    fitted_models_lgb.append(model)\n",
    "    y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores_lgb.append(auc_score)\n",
    "    y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_valid]\n",
    "    test(y_valid, y_pred_binary)\n",
    "    accuracy = accuracy_score(y_valid, y_pred_binary)\n",
    "    mae_score_lgb = mean_absolute_error(y_valid, y_pred_valid)\n",
    "    rmse_score_lgb = mean_squared_error(y_valid, y_pred_valid, squared=False)\n",
    "    r2_score_lgb = r2_score(y_valid, y_pred_valid)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"mae score: \", mae_score_lgb)\n",
    "    print(\"rmse score: \", rmse_score_lgb)\n",
    "    print(\"r2 score: \", r2_score_lgb)\n",
    "    \n",
    "    \n",
    "print(\"CV AUC scores: \", cv_scores_cat)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_cat))\n",
    "\n",
    "\n",
    "print(\"CV AUC scores: \", cv_scores_lgb)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f915e750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:16.615110Z",
     "iopub.status.busy": "2024-05-11T10:01:16.614403Z",
     "iopub.status.idle": "2024-05-11T10:01:16.621388Z",
     "shell.execute_reply": "2024-05-11T10:01:16.619922Z"
    },
    "papermill": {
     "duration": 0.049161,
     "end_time": "2024-05-11T10:01:16.625049",
     "exception": false,
     "start_time": "2024-05-11T10:01:16.575888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# fitted_models_cat = []\n",
    "# fitted_models_lgb = []\n",
    "\n",
    "# cv_scores_cat = []\n",
    "# cv_scores_lgb = []\n",
    "\n",
    "\n",
    "# for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n",
    "#     X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n",
    "#     X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "#     train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n",
    "#     val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n",
    "#     clf = CatBoostClassifier(\n",
    "#     eval_metric='AUC',\n",
    "#     learning_rate=0.03,\n",
    "#     iterations=200)\n",
    "#     random_seed=3107\n",
    "#     clf.fit(train_pool, eval_set=val_pool,verbose=300)\n",
    "#     fitted_models_cat.append(clf)\n",
    "#     y_pred_valid = clf.predict_proba(X_valid)[:,1]\n",
    "#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "#     cv_scores_cat.append(auc_score)\n",
    "    \n",
    "    \n",
    "#     X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n",
    "#     X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n",
    "    \n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         eval_set = [(X_valid, y_valid)],\n",
    "#         callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "    \n",
    "#     fitted_models_lgb.append(model)\n",
    "#     y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "#     cv_scores_lgb.append(auc_score)\n",
    "    \n",
    "    \n",
    "# print(\"CV AUC scores: \", cv_scores_cat)\n",
    "# print(\"Maximum CV AUC score: \", max(cv_scores_cat))\n",
    "\n",
    "\n",
    "# print(\"CV AUC scores: \", cv_scores_lgb)\n",
    "# print(\"Maximum CV AUC score: \", max(cv_scores_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a863ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:16.701425Z",
     "iopub.status.busy": "2024-05-11T10:01:16.700777Z",
     "iopub.status.idle": "2024-05-11T10:01:16.712127Z",
     "shell.execute_reply": "2024-05-11T10:01:16.710327Z"
    },
    "papermill": {
     "duration": 0.053995,
     "end_time": "2024-05-11T10:01:16.715685",
     "exception": false,
     "start_time": "2024-05-11T10:01:16.661690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        X[cat_cols] = X[cat_cols].astype(\"category\")\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_cat+fitted_models_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019b40d",
   "metadata": {
    "papermill": {
     "duration": 0.034703,
     "end_time": "2024-05-11T10:01:16.788630",
     "exception": false,
     "start_time": "2024-05-11T10:01:16.753927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validating the performance on the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9eda88e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:16.862471Z",
     "iopub.status.busy": "2024-05-11T10:01:16.861799Z",
     "iopub.status.idle": "2024-05-11T10:01:19.281503Z",
     "shell.execute_reply": "2024-05-11T10:01:19.279857Z"
    },
    "papermill": {
     "duration": 2.459251,
     "end_time": "2024-05-11T10:01:19.285023",
     "exception": false,
     "start_time": "2024-05-11T10:01:16.825772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = X_test\n",
    "df_train[cat_cols] = df_train[cat_cols].astype(str)\n",
    "df_train[cat_cols] = df_train[cat_cols].astype(\"category\")\n",
    "y_valid = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0b923a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:19.358653Z",
     "iopub.status.busy": "2024-05-11T10:01:19.358162Z",
     "iopub.status.idle": "2024-05-11T10:01:36.422242Z",
     "shell.execute_reply": "2024-05-11T10:01:36.419980Z"
    },
    "papermill": {
     "duration": 17.105083,
     "end_time": "2024-05-11T10:01:36.426228",
     "exception": false,
     "start_time": "2024-05-11T10:01:19.321145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[11148  3313]\n",
      " [ 3669 10667]]\n",
      "Outcome values : \n",
      " 11148 3313 3669 10667\n",
      "Classification reportÂ :Â \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.77      0.76     14461\n",
      "           0       0.76      0.74      0.75     14336\n",
      "\n",
      "    accuracy                           0.76     28797\n",
      "   macro avg       0.76      0.76      0.76     28797\n",
      "weighted avg       0.76      0.76      0.76     28797\n",
      "\n",
      "AUC score: 0.8374193180920111\n",
      "Accuracy: 0.7575441886307601\n",
      "mae score:  0.3394647049860567\n",
      "rmse score:  0.4054277987098332\n",
      "r2 score:  0.3425008115753131\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = model.predict_proba(df_train)[:,1]\n",
    "auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred_valid]\n",
    "test(y_valid, y_pred_binary)\n",
    "accuracy = accuracy_score(y_valid, y_pred_binary)\n",
    "mae_score_lgb = mean_absolute_error(y_valid, y_pred_valid)\n",
    "rmse_score_lgb = mean_squared_error(y_valid, y_pred_valid, squared=False)\n",
    "r2_score_lgb = r2_score(y_valid, y_pred_valid)\n",
    "print(\"AUC score:\", auc_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"mae score: \", mae_score_lgb)\n",
    "print(\"rmse score: \", rmse_score_lgb)\n",
    "print(\"r2 score: \", r2_score_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8de5e43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:36.501644Z",
     "iopub.status.busy": "2024-05-11T10:01:36.501244Z",
     "iopub.status.idle": "2024-05-11T10:01:36.507694Z",
     "shell.execute_reply": "2024-05-11T10:01:36.506321Z"
    },
    "papermill": {
     "duration": 0.048631,
     "end_time": "2024-05-11T10:01:36.510124",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.461493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"metric\": \"auc\",\n",
    "#     \"max_depth\": 10,  \n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"n_estimators\": 2000,  \n",
    "#     \"colsample_bytree\": 0.8,\n",
    "#     \"colsample_bynode\": 0.8,\n",
    "#     \"verbose\": -1,\n",
    "#     \"random_state\": 42,\n",
    "#     \"reg_alpha\": 0.1,\n",
    "#     \"reg_lambda\": 10,\n",
    "#     \"extra_trees\":True,\n",
    "#     'num_leaves':64,\n",
    "# }\n",
    "\n",
    "# fitted_models = []\n",
    "# cv_scores = []\n",
    "\n",
    "\n",
    "# for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#   Because it takes a long time to divide the data set, \n",
    "#     X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# each time the data set is divided, two models are trained to each other twice, which saves time.\n",
    "#     X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "#     model = lgb.LGBMClassifier(**params)\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         eval_set = [(X_valid, y_valid)],\n",
    "#         callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "#     fitted_models.append(model)\n",
    "#     y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "#     auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "#     cv_scores.append(auc_score)\n",
    "    \n",
    "# print(\"CV AUC scores: \", cv_scores)\n",
    "# print(\"Maximum CV AUC score: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db123027",
   "metadata": {
    "papermill": {
     "duration": 0.036049,
     "end_time": "2024-05-11T10:01:36.580379",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.544330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b2e14",
   "metadata": {
    "papermill": {
     "duration": 0.035103,
     "end_time": "2024-05-11T10:01:36.651233",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.616130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a5b1e",
   "metadata": {
    "papermill": {
     "duration": 0.037758,
     "end_time": "2024-05-11T10:01:36.726011",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.688253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b8c2b4",
   "metadata": {
    "papermill": {
     "duration": 0.035919,
     "end_time": "2024-05-11T10:01:36.797219",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.761300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.2 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6d73940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:36.870918Z",
     "iopub.status.busy": "2024-05-11T10:01:36.870446Z",
     "iopub.status.idle": "2024-05-11T10:01:36.875787Z",
     "shell.execute_reply": "2024-05-11T10:01:36.874060Z"
    },
    "papermill": {
     "duration": 0.045804,
     "end_time": "2024-05-11T10:01:36.878895",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.833091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "# auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fe3c0",
   "metadata": {
    "papermill": {
     "duration": 0.034629,
     "end_time": "2024-05-11T10:01:36.948652",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.914023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.3 GINI Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946fd09",
   "metadata": {
    "papermill": {
     "duration": 0.034912,
     "end_time": "2024-05-11T10:01:37.017560",
     "exception": false,
     "start_time": "2024-05-11T10:01:36.982648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ef3bb2",
   "metadata": {
    "papermill": {
     "duration": 0.037684,
     "end_time": "2024-05-11T10:01:37.092659",
     "exception": false,
     "start_time": "2024-05-11T10:01:37.054975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a55c87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:37.165592Z",
     "iopub.status.busy": "2024-05-11T10:01:37.165143Z",
     "iopub.status.idle": "2024-05-11T10:01:37.184727Z",
     "shell.execute_reply": "2024-05-11T10:01:37.183393Z"
    },
    "papermill": {
     "duration": 0.060333,
     "end_time": "2024-05-11T10:01:37.188127",
     "exception": false,
     "start_time": "2024-05-11T10:01:37.127794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop(columns=[\"WEEK_NUM\"])\n",
    "df_test = df_test.set_index(\"case_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "691d2972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:37.258472Z",
     "iopub.status.busy": "2024-05-11T10:01:37.258059Z",
     "iopub.status.idle": "2024-05-11T10:01:37.386956Z",
     "shell.execute_reply": "2024-05-11T10:01:37.385477Z"
    },
    "papermill": {
     "duration": 0.167867,
     "end_time": "2024-05-11T10:01:37.390216",
     "exception": false,
     "start_time": "2024-05-11T10:01:37.222349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test[cat_cols] = df_test[cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c8a71d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:01:37.471345Z",
     "iopub.status.busy": "2024-05-11T10:01:37.470568Z",
     "iopub.status.idle": "2024-05-11T10:01:38.672161Z",
     "shell.execute_reply": "2024-05-11T10:01:38.670321Z"
    },
    "papermill": {
     "duration": 1.251236,
     "end_time": "2024-05-11T10:01:38.675975",
     "exception": false,
     "start_time": "2024-05-11T10:01:37.424739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>0.168040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>0.487776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>0.148497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>0.226961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>0.760877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>0.317980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>0.356183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>0.118311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>0.329008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>0.244189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "case_id          \n",
       "57543    0.168040\n",
       "57549    0.487776\n",
       "57551    0.148497\n",
       "57552    0.226961\n",
       "57569    0.760877\n",
       "57630    0.317980\n",
       "57631    0.356183\n",
       "57632    0.118311\n",
       "57633    0.329008\n",
       "57634    0.244189"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "df_subm"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30685,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8821.976804,
   "end_time": "2024-05-11T10:01:42.374297",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-11T07:34:40.397493",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
